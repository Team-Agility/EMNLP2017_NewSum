utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances - to compute the match with the keywords accurately
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords, filler_words=filler_words, is_trad_doc=is_trad_doc)$utterances
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:7, .packages = c('igraph','SnowballC','hash','stringr'))  %dopar% {
#nrow(grid_parameter_tr)
my_row = grid_parameter_tr[k,]
#method = my_row['method']
#w = as.numeric(my_row['w'])
scaling_factor = as.numeric(my_row['scaling_factor'])
lambda = as.numeric(my_row['lambda'])
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method='TR', use_elbow=FALSE, use_percentage=NA, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output
my_summary = from_keyword_to_summary_submodularity(graph_keywords_scores_temp = keywords_scores, utterances = utterances, start_time = start_time, to_stem=T, max_summary_length=100, scaling_factor=scaling_factor, weighted_sum_concepts=T, negative_terms=FALSE, lambda=lambda, is_trad_doc=is_trad_doc)$my_summary
setwd(root_wd)
# delete temporary directory
unlink(temp_wd,recursive=T)
# write summary
writeLines(my_summary, paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/',method,'_',w,'_',scaling_factor,'_',lambda,'_',doc_id,'_system.txt'))
}
stopCluster(cl)
cat(doc_names[idx],'done in',proc.time() - t)
cat('\n')
}
=1
k=1
scaling_factor = as.numeric(my_row['scaling_factor'])
lambda = as.numeric(my_row['lambda'])
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method='TR', use_elbow=FALSE, use_percentage=NA, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output
scaling_factor = as.numeric(my_row['scaling_factor'])
lambda = as.numeric(my_row['lambda'])
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method='TR', use_elbow=FALSE, use_percentage=TRUE, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output # changed" use_percentage (NA->T), degeneracy ('weighted_k_core'->''), method ('CRP' -> 'TR')
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:7, .packages = c('igraph','SnowballC','hash','stringr'))  %dopar% {
#nrow(grid_parameter_tr)
my_row = grid_parameter_tr[k,]
#method = my_row['method']
#w = as.numeric(my_row['w'])
scaling_factor = as.numeric(my_row['scaling_factor'])
lambda = as.numeric(my_row['lambda'])
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method='TR', use_elbow=FALSE, use_percentage=TRUE, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output # changed" use_percentage (NA->T), degeneracy ('weighted_k_core'->''), method ('CRP' -> 'TR')
my_summary = from_keyword_to_summary_submodularity(graph_keywords_scores_temp = keywords_scores, utterances = utterances, start_time = start_time, to_stem=T, max_summary_length=100, scaling_factor=scaling_factor, weighted_sum_concepts=T, negative_terms=FALSE, lambda=lambda, is_trad_doc=is_trad_doc)$my_summary
setwd(root_wd)
# delete temporary directory
unlink(temp_wd,recursive=T)
# write summary
writeLines(my_summary, paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/',method,'_',w,'_',scaling_factor,'_',lambda,'_',doc_id,'_system.txt'))
}
stopCluster(cl)
for (idx in dev_set_idxs){
doc_id = unlist(strsplit(doc_names[idx],split='\\.'))[1]
raw_text = readLines(paste0(root_wd,'/to_summarize/',doc_names[idx]))
solution_summary = raw_text[2]
# write solution summary to corresponding ROUGE
writeLines(solution_summary, paste0(root_wd,'rouge2.0-distribution/reference_raw/',doc_id,'_reference.txt'))
original_doc = paste(raw_text[4:length(raw_text)],collapse=' ')
# split into sentences
s = as.String(original_doc)
boundaries = annotate(s, sta)
sent = s[boundaries]
l_sent = length(sent)
# retain only columns of interest
asr_info = cbind.data.frame(seq(1:l_sent),seq(1:l_sent)+2,rep('fake',l_sent),sent,stringsAsFactors = FALSE)
colnames(asr_info) = c("start","end","role","text")
cleaned_transcript = cleaning_transcript(my_transcript_df = asr_info, time_prune = 0.85, custom = custom_stopwords, pos=FALSE, to_stem=TRUE, overlap_threshold = 1.5, detected_language=detected_language, is_trad_doc=is_trad_doc)
cleaned_transcript_df = cleaned_transcript$collapse_output
cleaned_transcript_df_processed = cleaned_transcript_df$reduced_my_df_proc
cleaned_transcript_df_unprocessed = cleaned_transcript_df$reduced_my_df_unproc
terms_list = list(processed = from_cleaned_transcript_to_terms_list(cleaned_transcript_df_processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(cleaned_transcript_df_unprocessed[,4])$terms_list_partial)
utterances = as.character(cleaned_transcript_df_unprocessed[,4])
start_time = as.numeric(cleaned_transcript_df_unprocessed[,1])
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances - to compute the match with the keywords accurately
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords, filler_words=filler_words, is_trad_doc=is_trad_doc)$utterances
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:nrow(grid_parameter_tr), .packages = c('igraph','SnowballC','hash','stringr'))  %dopar% {
my_row = grid_parameter_tr[k,]
#method = my_row['method']
#w = as.numeric(my_row['w'])
scaling_factor = as.numeric(my_row['scaling_factor'])
lambda = as.numeric(my_row['lambda'])
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method='TR', use_elbow=FALSE, use_percentage=TRUE, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output # changed" use_percentage (NA->T), degeneracy ('weighted_k_core'->''), method ('CRP' -> 'TR')
my_summary = from_keyword_to_summary_submodularity(graph_keywords_scores_temp = keywords_scores, utterances = utterances, start_time = start_time, to_stem=T, max_summary_length=100, scaling_factor=scaling_factor, weighted_sum_concepts=T, negative_terms=FALSE, lambda=lambda, is_trad_doc=is_trad_doc)$my_summary
setwd(root_wd)
# delete temporary directory
unlink(temp_wd,recursive=T)
# write summary
writeLines(my_summary, paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/',method,'_',w,'_',scaling_factor,'_',lambda,'_',doc_id,'_system.txt'))
}
stopCluster(cl)
cat(doc_names[idx],'done in',proc.time() - t)
cat('\n')
}
warnings()
dim(grid_parameter_tr)
1491*15
system_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/'))
length(system_summary_names)
k=1
system_summary_names[k]
system_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/'))
system_summary_names[k]
method
original_name = system_summary_names[k]
elts = unlist(strsplit(original_name,split='_'))
elts
rev(elts)[2]
gsub('\\.',':',paste(elts[1:4],collapse='-'))
gsub('\\.',':',paste(elts[3:4],collapse='-'))
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(system_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/summaries_raw_pagerank/',system_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
original_name = system_summary_names[k]
elts = unlist(strsplit(original_name,split='_'))
new_name = paste(rev(elts)[2],gsub('\\.',':',paste(elts[3:4],collapse='-')),'system.txt',sep='_')
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/system/',new_name))
}
stopCluster(cl)
proc.time() - t
solution_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/reference_raw/'))
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(solution_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/reference_raw/',solution_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/reference/',solution_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
# compute ROUGE scores
setwd(paste0(root_wd,'/rouge2.0-distribution/'))
command = 'java -jar "rouge2.0.jar"'
command_unix = 'java -jar rouge2.0.jar'
if (operating_system=='unix'){
# calls to a Linux environment - shinyapps
system(paste0(command_unix))
} else if (operating_system=='windows'){
system(paste('cmd.exe /c', command), intern = FALSE, wait = TRUE)
}
# read back results
rouge_scores = read.csv("results_with_stopwords.csv", header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
dim(rouge_scores)
length(system_summary_names)
length(list.files(paste0(root_wd,'rouge2.0-distribution/test-summarization/system/')))
# compute ROUGE scores
setwd(paste0(root_wd,'/rouge2.0-distribution/'))
command = 'java -jar "rouge2.0.jar"'
command_unix = 'java -jar rouge2.0.jar'
if (operating_system=='unix'){
# calls to a Linux environment - shinyapps
system(paste0(command_unix))
} else if (operating_system=='windows'){
system(paste('cmd.exe /c', command), intern = FALSE, wait = TRUE)
}
rouge_scores = read.csv("results.csv", header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
rouge_scores = read.csv("results.csv", header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
dim(rouge_scores)
average_scores = list()
for (r in 1:nrow(grid_parameter)){
my_row = grid_parameter[r,]
#method = my_row['method']
#w = as.character(my_row['w'])
scaling_factor = as.character(my_row['scaling_factor'])
lambda = as.character(my_row['lambda'])
temp = rouge_scores[rouge_scores[,'System.Name']==paste(gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
#temp = rouge_scores[rouge_scores[,'System.Name']==paste(method,w,gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
print(dim(temp))
average_scores[[r]] = mean(temp[,'Avg_F.Score'])
if (r %% round(nrow(grid_parameter)/10) == 0){
print(paste0('==========',r,'=========='))
}
}
average_scores = unlist(average_scores)
class(average_scores)
to_write = cbind(grid_parameter[order(average_scores,decreasing=TRUE),],round(average_scores[order(average_scores,decreasing=TRUE)]*100,2))
colnames(to_write) = c('method','w','scaling_factor','lambda','ROUGE F-1')
write.table(to_write, paste0(root_wd,'/results/grid_search_results_pagerank.csv'),row.names=FALSE,col.names=T,quote=FALSE,sep=',')
head(grid_parameter_tr)
w
dim(grid_parameter_tr)
1491*15
elts
rev(elts)[2]
gsub('\\.',':',paste(elts[3:4],collapse='-'))
head(rouge_scores)
average_scores = list()
for (r in 1:nrow(grid_parameter_k)){
my_row = grid_parameter_k[r,]
#method = my_row['method']
#w = as.character(my_row['w'])
scaling_factor = as.character(my_row['scaling_factor'])
lambda = as.character(my_row['lambda'])
temp = rouge_scores[rouge_scores[,'System.Name']==paste(gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
#temp = rouge_scores[rouge_scores[,'System.Name']==paste(method,w,gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
print(dim(temp))
average_scores[[r]] = mean(temp[,'Avg_F.Score'])
if (r %% round(nrow(grid_parameter)/10) == 0){
print(paste0('==========',r,'=========='))
}
}
average_scores = unlist(average_scores)
average_scores = list()
for (r in 1:nrow(grid_parameter_tr)){
my_row = grid_parameter_tr[r,]
#method = my_row['method']
#w = as.character(my_row['w'])
scaling_factor = as.character(my_row['scaling_factor'])
lambda = as.character(my_row['lambda'])
temp = rouge_scores[rouge_scores[,'System.Name']==paste(gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
#temp = rouge_scores[rouge_scores[,'System.Name']==paste(method,w,gsub('\\.',':',scaling_factor),gsub('\\.',':',lambda), sep='-'),]
print(dim(temp))
average_scores[[r]] = mean(temp[,'Avg_F.Score'])
if (r %% round(nrow(grid_parameter)/10) == 0){
print(paste0('==========',r,'=========='))
}
}
average_scores = unlist(average_scores)
to_write = cbind(grid_parameter_tr[order(average_scores,decreasing=TRUE),],round(average_scores[order(average_scores,decreasing=TRUE)]*100,2))
colnames(to_write) = c('method','w','scaling_factor','lambda','ROUGE F-1')
write.table(to_write, paste0(root_wd,'/results/grid_search_results_pagerank.csv'),row.names=FALSE,col.names=T,quote=FALSE,sep=',')
to_write = cbind(grid_parameter_tr[order(average_scores,decreasing=TRUE),],round(average_scores[order(average_scores,decreasing=TRUE)]*100,2))
colnames(to_write) = c('scaling_factor','lambda','ROUGE F-1')#c('method','w','scaling_factor','lambda','ROUGE F-1')
write.table(to_write, paste0(root_wd,'/results/grid_search_results_pagerank.csv'),row.names=FALSE,col.names=T,quote=FALSE,sep=',')
test_set_idxs = setdiff(1:length(doc_names),dev_set_idxs)
# sanity check
#length(test_set_idxs) + length(dev_set_idxs) == length(doc_names)
t = proc.time()
for(k in 1:length(test_set_idxs)){
idx = test_set_idxs[k]
doc_id = unlist(strsplit(doc_names[idx],split='\\.'))[1]
raw_text = readLines(paste0(root_wd,'/to_summarize/',doc_names[idx]))
solution_summary = raw_text[2]
# write solution summary to corresponding ROUGE
writeLines(solution_summary, paste0(root_wd,'rouge2.0-distribution/reference_raw_test/',doc_id,'_reference.txt'))
original_doc = paste(raw_text[4:length(raw_text)],collapse=' ')
# split into sentences
s = as.String(original_doc)
boundaries = annotate(s, sta)
sent = s[boundaries]
l_sent = length(sent)
# retain only columns of interest
asr_info = cbind.data.frame(seq(1:l_sent),seq(1:l_sent)+2,rep('fake',l_sent),sent,stringsAsFactors = FALSE)
colnames(asr_info) = c("start","end","role","text")
cleaned_transcript = cleaning_transcript(my_transcript_df = asr_info, time_prune = 0.85, custom = custom_stopwords, pos=FALSE, to_stem=TRUE, overlap_threshold = 1.5, detected_language=detected_language, is_trad_doc=is_trad_doc)
cleaned_transcript_df = cleaned_transcript$collapse_output
cleaned_transcript_df_processed = cleaned_transcript_df$reduced_my_df_proc
cleaned_transcript_df_unprocessed = cleaned_transcript_df$reduced_my_df_unproc
terms_list = list(processed = from_cleaned_transcript_to_terms_list(cleaned_transcript_df_processed[,4])$terms_list_partial, unprocessed = from_cleaned_transcript_to_terms_list(cleaned_transcript_df_unprocessed[,4])$terms_list_partial)
utterances = as.character(cleaned_transcript_df_unprocessed[,4])
start_time = as.numeric(cleaned_transcript_df_unprocessed[,1])
# prune out utterances that are too short
utterances_lengthes = unlist(lapply(utterances, function(x){length(setdiff(unlist(strsplit(tolower(x),split=" ")), custom_stopwords))}))
index_remove = which(utterances_lengthes<=3)
if (length(index_remove)>0){
utterances = utterances[-index_remove]
start_time = start_time[-index_remove]
}
# clean utterances - to compute the match with the keywords accurately
utterances = clean_utterances(utterances, my_stopwords=custom_stopwords, filler_words=filler_words, is_trad_doc=is_trad_doc)$utterances
method = 'TR'
w = 6
scaling_factor = 0.9 #scaling_factor = 0.4
lambda = 1.4 #lambda = 1.2
# create temporary directory
temp_wd = paste0(root_wd,'/temp/',paste0(paste(sample(LETTERS, 5, TRUE),collapse=''),paste(sample(1:10,5,TRUE),collapse='')))
dir.create(temp_wd)
# copy necessary files to the temporary directory
to_copy = c("assign_attributes_to_graph_nocomm.R", "assign_attributes_to_graph_initial.R","best_level_density.R","cores_dec.R","concept_submodularity_objective.R","from_terms_to_graph.R","from_terms_to_keywords.R","from_terms_to_summary.R","from_keyword_to_summary_submodularity.R","get_elbow_point.R","kcore-1.0.jar","keyword_extraction.R","keyword_extraction_inner.R","sentence_extraction_submodularity.R","sentence_selection_greedy.R")
to_copy = unlist(lapply(to_copy, function(x){if(length(unlist(strsplit(x, split='\\.')))>1){return(x)}}))
file.copy(from=paste0(root_wd,'/',to_copy),to=temp_wd, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
setwd(temp_wd)
keywords_scores = from_terms_to_keywords(terms_list=terms_list, window_size=w, to_overspan=T, to_build_on_processed=T, community_algo="none", weighted_comm=NA, directed_comm=NA, rw_length=NULL, size_threshold=NULL, degeneracy="", directed_mode="all", method=method, use_elbow=FALSE, use_percentage=TRUE, percentage=0.15, number_to_retain=NA, which_nodes="all", overall_wd=temp_wd)$output
for (my_summary_size in seq(50,300,by=25)){
my_summary = from_keyword_to_summary_submodularity(graph_keywords_scores_temp = keywords_scores, utterances = utterances, start_time = start_time, to_stem=T, max_summary_length=my_summary_size, scaling_factor=scaling_factor, weighted_sum_concepts=T, negative_terms=FALSE, lambda=lambda, is_trad_doc=is_trad_doc)$my_summary
# write summary
writeLines(my_summary, paste0(root_wd,'rouge2.0-distribution/summaries_raw_test_pagerank/',doc_id,'_',my_summary_size,'-system.txt'))
}
setwd(root_wd)
# delete temporary directory
unlink(temp_wd,recursive=T)
print(k)
}
system_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/summaries_raw_test_pagerank/'))
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(system_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/summaries_raw_test_pagerank/',system_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/system/',system_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
solution_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/reference_raw_test/'))
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(solution_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/reference_raw_test/',solution_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/reference/',solution_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
# compute ROUGE scores
setwd(paste0(root_wd,'/rouge2.0-distribution/'))
command = 'java -jar "rouge2.0.jar"'
command_unix = 'java -jar rouge2.0.jar'
if (operating_system=='unix'){
# calls to a Linux environment - shinyapps
system(paste0(command_unix))
} else if (operating_system=='windows'){
system(paste('cmd.exe /c', command), intern = FALSE, wait = TRUE)
}
rouge_scores = read.csv('results.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
dim(rouge_scores)
11*15
182*11
length(seq(50,300,by=25))
length(test_set_idxs)
282*11
# read back results
rouge_scores = read.csv('results.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
#setwd(root_wd)
# compute average precision, recall and F-1 score for each summary size
results = list()
counter = 1
for (my_summary_size in seq(50,300,by=25)){
results[[counter]] = apply(rouge_scores[grepl(my_summary_size,rouge_scores[,'System.Name']),c('Avg_Recall','Avg_Precision','Avg_F.Score')],2,mean)
counter = counter + 1
}
names(results) = seq(50,300,by=25)
results
rouge_scores = read.csv('results_test_pagerank.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
#setwd(root_wd)
# compute average precision, recall and F-1 score for each summary size
results_pr = list()
counter = 1
for (my_summary_size in seq(50,300,by=25)){
results_pr[[counter]] = apply(rouge_scores[grepl(my_summary_size,rouge_scores[,'System.Name']),c('Avg_Recall','Avg_Precision','Avg_F.Score')],2,mean)
counter = counter + 1
}
names(results_pr) = seq(50,300,by=25)
# read back results
rouge_scores = read.csv('results_test_with_stopwords.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
#setwd(root_wd)
# compute average precision, recall and F-1 score for each summary size
results = list()
counter = 1
for (my_summary_size in seq(50,300,by=25)){
results[[counter]] = apply(rouge_scores[grepl(my_summary_size,rouge_scores[,'System.Name']),c('Avg_Recall','Avg_Precision','Avg_F.Score')],2,mean)
counter = counter + 1
}
names(results) = seq(50,300,by=25)
results
getwd()
rouge_scores = read.csv('results_test_with_stopwords.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
results = list()
counter = 1
for (my_summary_size in seq(50,300,by=25)){
results[[counter]] = apply(rouge_scores[grepl(my_summary_size,rouge_scores[,'System.Name']),c('Avg_Recall','Avg_Precision','Avg_F.Score')],2,mean)
counter = counter + 1
}
names(results) = seq(50,300,by=25)
results
dim(rouge_scores)
282*11
system_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/summaries_raw_test/'))
length(system_s)
length(system_summary_names)
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(system_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/summaries_raw_test_pagerank/',system_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/system/',system_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(system_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/summaries_raw_test/',system_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/system/',system_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
solution_summary_names = list.files(paste0(root_wd,'rouge2.0-distribution/reference_raw_test/'))
t = proc.time()
nc = detectCores() - 1
cl = makeCluster(nc)
registerDoParallel(cl)
foreach(k = 1:length(solution_summary_names), .packages = c('stringr')) %dopar% {
X = readLines(paste0(root_wd,'rouge2.0-distribution/reference_raw_test/',solution_summary_names[k]))
X_cleaned = unlist(lapply(X, clean_summary))
writeLines(X_cleaned, paste0(root_wd,'rouge2.0-distribution/test-summarization/reference/',solution_summary_names[k]))
}
stopCluster(cl)
proc.time() - t
# compute ROUGE scores
setwd(paste0(root_wd,'/rouge2.0-distribution/'))
command = 'java -jar "rouge2.0.jar"'
command_unix = 'java -jar rouge2.0.jar'
if (operating_system=='unix'){
# calls to a Linux environment - shinyapps
system(paste0(command_unix))
} else if (operating_system=='windows'){
system(paste('cmd.exe /c', command), intern = FALSE, wait = TRUE)
}
# compute ROUGE scores
setwd(paste0(root_wd,'/rouge2.0-distribution/'))
command = 'java -jar "rouge2.0.jar"'
command_unix = 'java -jar rouge2.0.jar'
if (operating_system=='unix'){
# calls to a Linux environment - shinyapps
system(paste0(command_unix))
} else if (operating_system=='windows'){
system(paste('cmd.exe /c', command), intern = FALSE, wait = TRUE)
}
rouge_scores = read.csv('results.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
dim(rouge_scores)
rouge_scores = read.csv('results.csv', header=TRUE) #[,c("Avg_Recall","Avg_Precision","Avg_F.Score")]
#setwd(root_wd)
# compute average precision, recall and F-1 score for each summary size
results = list()
counter = 1
for (my_summary_size in seq(50,300,by=25)){
results[[counter]] = apply(rouge_scores[grepl(my_summary_size,rouge_scores[,'System.Name']),c('Avg_Recall','Avg_Precision','Avg_F.Score')],2,mean)
counter = counter + 1
}
names(results) = seq(50,300,by=25)
results
results_pr
results_pr_df = matrix(unlist(results_pr), byrow=T, nrow=length(results_pr))
results_pr_df
results_pr_df = as.data.frame(matrix(unlist(results_pr), byrow=T, nrow=length(results_pr)))
rownames(results_pr_df) = seq(50,300,by=25)
results_df = as.data.frame(matrix(unlist(results), byrow=T, nrow=length(results)))
rownames(results_df) = seq(50,300,by=25)
results_df
results_pr_df
